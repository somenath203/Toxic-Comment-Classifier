# Toxic Comment Classifier

This is a project which categorizes a particular toxic comment entered by the user into mainly 6 categories and those are: `toxic`, `severe_toxic`, `obscene`, `threat`, `insult` and `identity_hate`.

## Tech Stack Used

From the frontend part, `flutter` is used, for the model part, the open-source huggingface model `toxic-bert` is used and for connecting `flutter` with `toxic-bert` model, `fastAPI` is used.

## Link to the HuggingFace Model

https://huggingface.co/unitary/toxic-bert

## Demo Video of the Project

https://github.com/somenath203/Toxic-Comment-Classifier/assets/81456073/6a92ced1-4eec-423e-83fb-f900118644ed
